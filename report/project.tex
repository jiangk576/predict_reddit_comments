\documentclass[11pt, twocolumn]{article}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{natbib}
\usepackage{url}
\renewcommand{\thesubsection}{\alph{subsection}.}

\begin{document}
\title{Predicting Comment Karma by Subreddit}
\author{Yoav Zimmerman (304125151) \\
	    CS 260: Machine Learning Algorithms \\}
\maketitle

\section{Abstract}

TODO: write after

\section{Motivation}

TODO: update [copied from proposal]

Voting systems are a standard part of many online communities. Whether the measure is ``likes'' on a facebook status, “hearts” on a picture in Instagram, or “upvotes” on a submission from Reddit, these voting systems often define which online content receives the most attention. Understanding the dynamics of user behavior in voting systems has applications in the world of online advertising, voting algorithm design, and viral marketing. In this project, we plan on examining the popularity of online content through Reddit \textit{comment karma}. Reddit is a website that aggregates link submissions, and allows users to either “upvote” or “downvote” other users comments on submissions. These votes are aggregated into a single, dynamic karma score for each Reddit comment. By attempting to formulate a model for predicting Reddit \textit{comment karma} on a given subreddit, we hope to gain insights into why and how comments go viral on that subreddit. Furthermore, a prediction function on comment karma for a given subreddit could provide an interesting similarity measure between it and other subreddits.

\section{Background}

Although there has some previous academic research on the subject of on the popularity of online content, there has been very little work exploring the prediction of the popularity of user-generated text, in particular. Szabo (http://www.hpl.hp.com/research/idl/papers/predictions/predictions.pdf) was able to forecast the long-term (30 day) popularity of Digg content with relative success using short-term (2 hour) user access and popularity data. There have been several student projects of specifically attempting to predict popularity of user-submitted comments of websites such as HackerNews and Reddit. Lamberson et. Al (TODO: reference) is a project that uses a large variety of features to attempt to predict scores of Reddit comments by using a prediction model. Lakkaraju is another student project that attempts predicting the score of top-level reddit submissions (note the difference from \textit{comments}). 

\section{Data}
	\subsection{Dataset}
	The dataset used in this project was using the publicly available dataset of 1.7 billion reddit comments (include reference). The strength of the dataset was in it's large size. The weakness was that the data associated each comment only consisted of basic features such as body text, author, time posted, and score of the comment. To collect more potentially important metadata such as the depth of a comment, a web crawler would be required to collect more data and augment this dataset. Since augmenting such a large dataset in this way is unpractical, this project aimed to focus mainly on features from the body text.
	
	\subsection{Features}
	The were N features extracted from each comment, most of which were extracted from the text of the comment itself. The main features are described below:
	\begin{enumerate}
		\item Bag of Words
		\item Counts (character count, word count)
		\item Average token length
		\item hour of time posted
		\item sentiment analysis (using AFINN-111.txt)
		\item number of punctuations (?, !)
	\end{enumerate}

\section{Models}
	There two types of learning models that we could apply to the features we came up
	
	\subsection{Regression}
	The most intutive model to predict the score of a comment given it's features is to use a regression model. An advantage of a regression model is it naturally maps well to the dataset, as the score labels associated with each comment are ``continuous'' integers. A Ridge Regression model was used, which learns a weight vector $\mathbf{w}$ over the following loss function:
	\begin{gather*}
		\sum^N{ (\mathbf{w^T} \mathbf{x_n} - \mathbf{y_n})^2} + \lambda \sum^N{ w }
	\end{gather*}
	where $\lambda$ is a hyperparameter tuned by cross-validation testing.
	
	\subsection{Classification}
	Another approach is to divide the comments into buckets based on their scores, and attempt to learn a multi-class classification model over these buckets. This model includes several complications that do not arise in the case of regression. First, the number of buckets introduces another ``hyperparameter'' into the algorithm- with a larger amount of buckets the model will have predict a finer granularity of scores, but too many buckets will result in a too difficult of a model to learn. In addition, the distribution of the dataset was heavily skewed towards lower scored comments in the range of 0-5. To address these complications, each comment was assigned to a bucket according to a log scale:
	\begin{gather*}
		bucket_n = \begin{cases}
				0 & score_n \le 0 \\
				log_2(score_n) + 1 & score_n \geq 0
				 \end{cases}
	\end{gather*}
	
	After the comments are put into buckets, a One-vs-Rest model can be used to construct $k-1$ classifiers over $k$ buckets. The classification algorithms experimented with were Logistic Regression and Naive Bayes. TODO: more classification algorithms?

\section{Implementation}
	The feature processing and learning models were implemented by the 

\section{Results and Evaluation}

/r/askreddit with buckets = vocabSize = 10000

pipeline, test error, best reg param
(bag,144.7044629037733, 0.03125)
(tfidf,147.2399698192555, 32)
(metadata,118.75863545502081, 32)

/r/movies with buckets = vocabSize = 10000

(bag,85.38828682328494, 32)
(tfidf,84.3943617278876, 32)
(metadata,53.43337526230372, 2)

/r/hiphopheads

(bag,36.673371574989766. 32)
(tfidf,36.71083116813651, 32)
(metadata,38.63094133654731, 0.03125)

/r/askscience

(bag,117.9463278092658)
(tfidf,122.12027363796126)
(metadata,116.28251182008567)


\section{Future Work}



\bibliographystyle{acm}
\bibliography{proposal}

\end{document}