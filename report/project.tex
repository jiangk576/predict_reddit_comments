\documentclass[11pt, twocolumn]{article}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{caption}
\captionsetup{font=small}
\renewcommand{\thesubsection}{\alph{subsection}.}

\begin{document}
\title{Predicting Comment Karma by Subreddit}
\author{Yoav Zimmerman (304125151) \\
	    CS 260: Machine Learning Algorithms \\}
\maketitle

\section{Abstract}

In this report, I approach the problem of predicting the score of a Reddit comment given the body text of the comment. To use as features, I experiment with a bag-of-words model, TF-IDF vectorization, and several more metadata features extracted from the text of each comment. I fit a Ridge Regression model on several different subreddits, achieving a 36.67 root squared mean test error on the subreddit \textit{/r/hiphopheads}. I achieve better results fitting Logistic Regression models, achieving an 82.4\% precision with a binary classification model on the subreddit \textit{/r/music}, an improvement of 7.4\% over the baseline model. I also achieve a 62.5\% precision with a multi-class classification model on \textit{/r/music}, an improvement of 13.4\% over the baseline model.

\section{Motivation}

Voting systems are a standard part of many online communities. Whether the measure is ``likes'' on a facebook status, “hearts” on a picture in Instagram, or “upvotes” on a submission from Reddit, these voting systems often define which online content receives the most attention. Understanding the dynamics of user behavior in voting systems has applications in the world of online advertising, voting algorithm design, and viral marketing. Reddit is one such website that aggregates link submissions, and allows users to either “upvote” or “downvote” other users comments on submissions. By attempting to formulate a text-based model for predicting Reddit \textit{comment karma} on a given subreddit, I hope to gain insights into what text elements make a comment ``good'' or ``bad''.

\section{Background}

Although there has some previous academic research on the subject of popularity of online content, there has been very little work exploring the prediction of the popularity of user-generated text such as reddit comments, in particular. Szabo \cite{predict_digg} was able to forecast the long-term (30 day) popularity of Digg content with relative success using short-term (2 hour) user access and popularity data. There have also been several student projects attempting to predict popularity of user-submitted comments of websites such as HackerNews and Reddit. Lamberson et. Al \cite{lamberson} is an example student project that uses a large variety of features to attempt to predict scores of Reddit comments by using a prediction model, with relative success focusing on features other than text.

\section{Data}
	\subsection{Dataset}
	The dataset used in this project was the publicly available dataset of 1.7 billion reddit comments \cite{dataset}. Although this dataset is very large, the features are relatively sparse; each comment only consists of basic features such as body text, author, time posted, and score of the comment. To narrow down to a more usuable data set, only comments from the month of September 2015 were used. Next comments were grouped by subreddit so that seperate models could be trained on each subreddit. Table \ref{size_table} shows the size of various subreddits used through this project.
	
	\begin{table}[h!]
	\centering
	\begin{tabular}{ | c | c |}
	\hline
	Subreddit & Number of Comments \\
	\hline
	\hline
	/r/askreddit & 1,000,000 \\
	\hline
	/r/videos & 550,192 \\
	\hline
	/r/movies & 288,522 \\
	\hline
	/r/hiphopheads & 137,363 \\
	\hline
	/r/music & 114,458 \\
	\hline
	/r/askscience & 27,747 \\
	\hline
	\end{tabular}
	\caption{Size of subreddit datasets}
	\label{size_table}
	\end{table}
	
	\subsection{Features}
	The most significant feature used in experiments was a vector representing the text body. First, the text body was tokenized on whitespace and stripped of non-alphanumeric characters. Two approaches were used to vectorize this list of tokens. One is a unigram \textbf{bag-of-words model} in which the value $c_i$ corresponds to the amount of time token $w_i$ appears in the comment body. Another representation often used on text corpus' is \textbf{Term-Frequency $\cdot$Inverse-Term-Frequency} (TFIDF), where the term frequency is the same counts value as in the bag-of-words model, and the inverse term frequency is a measure of how much information a ``word'' provides.
	
	\begin{gather*}
		idf(t) = log \bigg( \frac{\sum_{d \in D}{1}}{\sum_{d \in D; t \in d}{1}} \bigg)
	\end{gather*}
		
	In addition to the body vector features, there were 7 metadata features extracted from each comment:
	\begin{enumerate}
		\setlength\itemsep{-0.3em}
		\item Character Count
		\item Token Count
		\item Average Token Length
		\item Sentiment Analysis using the AFINN-111 dataset \cite{sentiment} 
		\item Number of question (?) marks
		\item Number of exclamation (!) marks
		\item Hour of Time posted
	\end{enumerate}

\section{Models}
	There were two families of models  applied to this problem.
	
	\subsection{Regression}
	The most intutive model to predict the score of a comment given it's features is to use a regression model. An advantage of a regression model is it naturally maps well to the dataset, as the score labels associated with each comment are continuous integers. The Ridge Regression algorithm was used, where the regularization parameter $\lambda$ was tuned by cross-validation testing.
	
	\subsection{Classification}
	Another approach to modeling this problem is to bucket the comments into $k$ score buckets and then run a classification algorithm on them. The two classification algorithms experimented with were Logistic Regression and Multinomial Naive Bayes. \\
	\\ 
	Classification does introduce a complication that does not arise in the case of regression. The number of buckets brings another ``hyperparameter'' into the algorithm- with a larger amount of buckets the model will have predict a finer granularity of scores, but too many buckets will result in a too difficult of a model to learn. 
		
		\subsubsection{Bucket Classification}
		To address the skewed nature of the dataset towards lower scores, comments were assigned to a score bucket according to a log scale.
		\begin{gather*}
			bucket_n = \begin{cases}
				0 & score_n < 0 \\
				log_2(score_n) + 1 & score_n \geq 0
				 \end{cases}
		\end{gather*}
	
	
		\subsubsection{Special Case: Binary Classification}
		Another approach was to decide on a score threshold to divide the dataset into ``positive'' and ``negative'' comments. Note that this is a special case of bucket classification above, where number of buckets is equal to 2. This simple model is the easiest to learn over, but gives us the least granularity in the task of predicting a new reddit comment.

\section{Implementation}
	The large size of the dataset proved to be the most challenging part of this project. The feature processing and learning models were implemented using the Apache Spark \cite{spark} distributed computing framework. The Spark framework was a good fit due to it's capabilities for distributed computation and that it comes with the MLLib API for Ridge Regression, Multinomial Naive Bayes, and Logistic Regression. Spark also comes with helper scripts to set up clusters to run on Amazon's EC2 cloud computing services. A cluster of 4 machines on Amazon EC2 were used to train models with greater than 300,000 comments. When transforming comments into unigram vectors, the vocabulary size was limited to a maximum of 50,000 of the most frequently used words for scalability reasons. Simillarly, for TF-IDF vectorization the hashing trick \cite{hashing} was used to restrict the feature vector to a maximum of 50,000 words. \\
	\\
	The code written for this project is open-source and publicly available on github.\footnote{\url{https://github.com/yoavz/predict_reddit_comments}}

\section{Results/Evaluation}

	\subsection{Regression}
	To evaluate the effectiveness of regression models, the \textbf{Root Mean Squared Error (RMSE)} can be used to compare test errors across models. Intuitively, this can be thought of as how far, on average, a score prediction is from the actual score of a comment.
	\begin{gather*}
		\sqrt{\cfrac{\sum^N{(\mathbf{w^T} \mathbf{x_n} - \mathbf{y_n})^2}}{N}}
	\end{gather*}
	
	Table \ref{reg_table} shows the testing RMSE after training and testing on 10,000 comment subset of four different subreddits. Each set of comments was split into 70\% training and 30\% testing. The three variants of feature vectors used were:
	\begin{enumerate}
		\setlength\itemsep{0em}
		\item Unigrams + Metadata features
		\item TF-IDF + Metadata features
		\item Only Metadata features
	\end{enumerate}
	
	\begin{table}[h!]
	\centering
	\begin{tabular}{  c | c  c  c | }
	\cline{2-4}
	 & \multicolumn{3}{ |c| } {Feature Variants} \\
	\hline
	\multicolumn{1}{|c|}{Subreddit} & Bag & TF-IDF & Metadata \\
	 \hline
	 \hline
	\multicolumn{1}{|c|}{/r/askreddit} & 144.70 & 147.24 & 118.76 \\
	\hline
	\multicolumn{1}{|c|}{/r/movies} & 85.39 & 84.39 & 53.43 \\
	\hline
	\multicolumn{1}{|c|}{/r/hiphopheads} & 36.67 & 36.71 & 38.63 \\
	\hline
	\multicolumn{1}{|c|}{/r/askscience} & 117.95 & 122.12 & 116.28 \\
	\hline
	\end{tabular}
	\caption{Linear Regression on 10k comment subsets}
	\label{reg_table}
	\end{table}

	None of these regression models achieve a satisfactory RMSE. The feature variant does not seem to have much of an effect; TF-IDF performs very similarly to unigrams, and metadata features even outperform both in some cases. With this small and noisy dataset, the models are \textit{severely overfitting} to the training set, as evidenced by Figure \ref{movies_plot_10k}, which shows training and testing RMSE against different values of regularization parameter $\lambda$.
	
	\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{movies_plot_10k.png}
	\caption{RMSE against $\lambda$ for /r/movies}
	\label{movies_plot_10k}
	\end{figure}
	
	Figure \ref{full_reg} shows the results of larger scale learning using the unigrams feature variant and the entire comment dataset for each subreddit. As can be seen, increasing the size of the dataset has very little effect on the final test RMSE of the model. In the case of the subreddit \textit{/r/askreddit}, for example, the test error actually increases, although the model is learned over 1,000,000 comments instead of the previous 10,000 comments.
	
	\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{full_reg.png}
	\caption{Linear Regression on full comment datasets}
	\label{full_reg}
	\end{figure}
	
	\subsection{Binary Classification}
	The metric used to evaluate the effectiveness of classification models was the \textbf{precision}, or the percentage of comments that are successfully assigned to the correct class by the trained model.
	
	\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{full_bin_class.png}
	\caption{Binary Classification}
	\label{full_bin_class}
	\end{figure}
	
	Figure \ref{full_bin_class} shows the precision of the Naive Bayes and Logistic Regression models on several different subreddits. It is also displayed next to the baseline model precision, which ignores the features and predicts a class based on the prior probabilities of the two classes. Logistic Regression consistently performed better than Naive Bayes and the baseline by a small amount. The best performing model is Logistic Regression on the \textit{/r/music} subreddit, achieving a 7.4\% improvement over the baseline model to achieve an 82.4\% precision.

	\subsection{Multi-Class Classification}
	
	\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{full_multi_class.png}
	\caption{Multi-Class Classification}
	\label{full_multi_class}
	\end{figure}

	Figure \ref{full_multi_class} shows the precision of the Naive Bayes and Logistic Regression (with the three feature variants) next to the baseline model on several subreddits. In all cases, both prediction models do better than the baseline. The most successful prediction model is a Logistic Regression on \textit{/r/music}, where it improves the precision of the baseline by 13.4\% to achieve a 62.5\% precision over 14 score buckets.
	
\section{Analysis}

	The performance of the models were varied, depending on subreddit. In general, the classification models performed better than the regression models, but neither performed spectacularly. There are several potential reasons for why this is. One reason may be that the simple tokenization and metadata feauteres do not correctly capture the inherent quality of a comment. The correlation between reddit comments and score may be mostly a function of other contextual comment features, or is embedded in a deeper semantics that are not properly conveyed by unigrams. Another potential reason is insufficient data; many ambitious text and image learning problems take days or weeks to train before any meaningful results are reached.

\section{Future Work}
	
	In future work, it would be useful to augment the data set with a richer set of features that includes context about the parent post and surrounding posts. For example, perhaps the score of a parent post or sibling post(s) are correlated with the score of the given comment. Although richer features such as context are not part of the 1.7 billion reddit comment dataset, it is possible to build a web crawler to access additional features. There are also richer features that could be introduced over the original dataset, such as word2vec \cite{word2vec}, which maps each token to a semantic vector space. Perhaps certain areas of the comment body, such as the last sentence or the first sentence, are more important in deciding the final score. Unfortunetely, these options could not be explored in this project due to time constraints.


\bibliographystyle{acm}
\bibliography{project}

\end{document}